{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, date, time, timedelta\n",
    "\n",
    "#for models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "\n",
    "#for PCA & PLS\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "\n",
    "#for Data\n",
    "import yfinance as yf\n",
    "\n",
    "#for Data Distribution\n",
    "from scipy.stats import kurtosis, skew, shapiro\n",
    "from scipy import stats\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Fetch historical data for the S&P 500\n",
    "sp500_data = yf.download('^GSPC', start='2002-01-01', end='2023-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "sp500_validation_set = yf.download('^GSPC', start='2023-01-02', end='2024-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check NA Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in DataFrame:\n",
      "Open         False\n",
      "High         False\n",
      "Low          False\n",
      "Close        False\n",
      "Adj Close    False\n",
      "Volume       False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in the data\n",
    "nan_values_df = sp500_data.isna().any()\n",
    "\n",
    "print(\"NaN values in DataFrame:\")\n",
    "print(nan_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Moving Average:\n",
    "#returns the dataframe with additional coumn of simple moving average\n",
    "def calculate_sma(df: pd.DataFrame, column: str = 'Adj Close', window_size: int = 15) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the Simple Moving Average (SMA) for a given column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the financial data.\n",
    "        column (str): Name of the column for which to calculate SMA. Default is 'close'.\n",
    "        window_size (int): Size of the moving window. Default is 15.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with SMA column added.\n",
    "    \"\"\"\n",
    "    # Calculate SMA\n",
    "    sma = df[column].rolling(window=window_size, min_periods=1).mean()\n",
    "    \n",
    "    # Create a DataFrame to store SMA\n",
    "    df['SMA'] = sma\n",
    "    df['SMA_signal'] = df['Close'] - df['SMA']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted Moving Average\n",
    "def calculate_wma(df: pd.DataFrame, column: str = 'Adj Close', window_size: int = 15) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the Weighted Moving Average (WMA) for a given column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the financial data.\n",
    "        column (str): Name of the column for which to calculate WMA. Default is 'close'.\n",
    "        window_size (int): Size of the moving window. Default is 15.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with WMA and WMA signal columns added.\n",
    "    \"\"\"\n",
    "    # Generate the weights\n",
    "    weights = np.arange(1, window_size + 1)\n",
    "    data = df[column]\n",
    "    \n",
    "    # Calculate the WMA using convolution\n",
    "    wma = data.rolling(window=window_size).apply(lambda prices: np.dot(prices, weights) / weights.sum(), raw=True)\n",
    "    \n",
    "    # Create a DataFrame to store WMA\n",
    "    df['WMA'] = wma\n",
    "    \n",
    "    # Add WMA signal column\n",
    "    df['WMA_signal'] = df[column] - wma\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MACD\n",
    "def calculate_macd(df: pd.DataFrame, short_window:int=12, long_window:int=26, signal_window:int=9, column: str = 'Adj Close') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the Moving Average Convergence Divergence (MACD) for a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "        short_window (int): The short-term window size for the short EMA.\n",
    "        long_window (int): The long-term window size for the long EMA.\n",
    "        signal_window (int): The window size for the signal line EMA.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with additional columns for MACD and signal line.\n",
    "    \"\"\"\n",
    "    # Calculate short-term EMA\n",
    "    short_ema = df[column].ewm(span=short_window, min_periods=1, adjust=False).mean()\n",
    "    \n",
    "    # Calculate long-term EMA\n",
    "    long_ema = df[column].ewm(span=long_window, min_periods=1, adjust=False).mean()\n",
    "    \n",
    "    # Calculate MACD line\n",
    "    macd_line = short_ema - long_ema\n",
    "    \n",
    "    # Calculate signal line\n",
    "    signal_line = macd_line.ewm(span=signal_window, min_periods=1, adjust=False).mean()\n",
    "    \n",
    "    # Store MACD and signal line in the DataFrame\n",
    "    df['MACD'] = macd_line\n",
    "    df['Signal Line'] = signal_line\n",
    "    df['macd_signal'] = macd_line - signal_line\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic_oscillator\n",
    "def calculate_stochastic_oscillator(df, k_fast_period=14, k_slow_period=3, d_slow_period=3, column: str = 'Adj Close'):\n",
    "    \"\"\"\n",
    "    Calculate the Stochastic Oscillator and its corresponding moving averages (K and D lines).\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "        k_fast_period (int): The period for the fast %K line.\n",
    "        k_slow_period (int): The period for the slow %K line.\n",
    "        d_slow_period (int): The period for the slow %D line.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with additional columns for %K_fast, %K_slow, %D_fast, and %D_slow.\n",
    "    \"\"\"\n",
    "    # Calculate highest high and lowest low over the period\n",
    "    HH = df['High'].rolling(window=k_fast_period).max()\n",
    "    LL = df['Low'].rolling(window=k_fast_period).min()\n",
    "\n",
    "    # Calculate %K_fast\n",
    "    df['%K_fast'] = ((df[column] - LL) / \n",
    "                     (HH - LL)) * 100\n",
    "    \n",
    "    # Calculate %K_slow (smoothed %K_fast)\n",
    "    df['%K_slow'] = df['%K_fast'].rolling(window=k_slow_period).mean()\n",
    "    \n",
    "    # Calculate %D_fast (3-day SMA of %K_slow)\n",
    "    df['%D_fast'] = df['%K_slow'].rolling(window=d_slow_period).mean()\n",
    "    \n",
    "    # Calculate %D_slow (3-day SMA of %D_fast)\n",
    "    df['%D_slow'] = df['%D_fast'].rolling(window=d_slow_period).mean()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RSI\n",
    "def calculate_rsi(df, window_size=14, column: str = 'Adj Close'):\n",
    "    \"\"\"\n",
    "    Calculate the Relative Strength Index (RSI) for a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "        window (int): The window size for calculating RSI.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with an additional column for RSI.\n",
    "    \"\"\"\n",
    "    # Calculate price changes\n",
    "    delta = df[column].diff()\n",
    "    \n",
    "    # Define up and down moves\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window_size).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window_size).mean()\n",
    "    \n",
    "    # Calculate the relative strength (RS)\n",
    "    rs = gain / loss\n",
    "    \n",
    "    # Calculate RSI\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Store RSI in the DataFrame\n",
    "    df['RSI'] = rsi\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WPR\n",
    "def calculate_williams_percent_r(df, window=14, column: str = 'Adj Close'):\n",
    "    \"\"\"\n",
    "    Calculate the Williams %R (WPR) for a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "        window (int): The window size for calculating WPR.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with an additional column for WPR.\n",
    "    \"\"\"\n",
    "    # Calculate highest high and lowest low over the window\n",
    "    highest_high = df['High'].rolling(window=window).max()\n",
    "    lowest_low = df['Low'].rolling(window=window).min()\n",
    "    \n",
    "    # Calculate Williams %R\n",
    "    wpr = ((highest_high - df[column]) / (highest_high - lowest_low)) * -100\n",
    "    \n",
    "    # Store WPR in the DataFrame\n",
    "    df['WPR'] = wpr\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bollinger Bands\n",
    "def calculate_bollinger_bands(df, window=20, num_std_dev=2, column: str = 'Adj Close'):\n",
    "    \"\"\"\n",
    "    Calculate Bollinger Bands for a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "        window (int): The window size for the moving average.\n",
    "        num_std_dev (int): The number of standard deviations for the bands.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with additional columns for Bollinger Bands.\n",
    "    \"\"\"\n",
    "    # Calculate rolling mean and standard deviation\n",
    "    rolling_mean = df[column].rolling(window=window).mean()\n",
    "    rolling_std = df[column].rolling(window=window).std()\n",
    "    \n",
    "    # Calculate upper and lower bands\n",
    "    upper_band = rolling_mean + (rolling_std * num_std_dev)\n",
    "    lower_band = rolling_mean - (rolling_std * num_std_dev)\n",
    "    \n",
    "    # Store Bollinger Bands in the DataFrame\n",
    "    df['Bollinger Upper'] = upper_band\n",
    "    df['Bollinger Lower'] = lower_band\n",
    "    df['Bollinger Diff'] = upper_band - lower_band\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On-Balance Volume (OBV)\n",
    "def calculate_obv(df, column: str = 'Adj Close'):\n",
    "    \"\"\"\n",
    "    Calculate On-Balance Volume (OBV) for a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with additional column for OBV.\n",
    "    \"\"\"\n",
    "    obv_values = []\n",
    "    prev_obv = 0\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        if df[column].iloc[i] > df[column].iloc[i - 1]:\n",
    "            obv = prev_obv + df['Volume'].iloc[i]\n",
    "        elif df[column].iloc[i] < df[column].iloc[i - 1]:\n",
    "            obv = prev_obv - df['Volume'].iloc[i]\n",
    "        else:\n",
    "            obv = prev_obv\n",
    "\n",
    "        obv_values.append(obv)\n",
    "        prev_obv = obv\n",
    "\n",
    "    # Add initial OBV value as 0\n",
    "    obv_values = [0] + obv_values\n",
    "\n",
    "    # Store OBV in the DataFrame\n",
    "    df['OBV'] = obv_values\n",
    "\n",
    "    # Convert OBV column to int64\n",
    "    df['OBV'] = df['OBV'].astype('int64')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average True Range (ATR)\n",
    "def calculate_atr(df, period=14):\n",
    "    \"\"\"\n",
    "    Calculate the Average True Range (ATR) of a stock dataset.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing stock data, with 'High', 'Low', and 'Close' columns representing high, low, and closing prices respectively.\n",
    "        period (int): Number of periods for which to calculate the ATR (default is 14).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with 'ATR' column containing the calculated ATR values.\n",
    "    \"\"\"\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "    close = df['Adj Close']\n",
    "    \n",
    "    # True Range (TR) calculation\n",
    "    df['TR'] = df[['High', 'Low', 'Adj Close']].apply(lambda row: max(row['High'] - row['Low'], abs(row['High'] - row['Adj Close']), abs(row['Low'] - row['Adj Close'])), axis=1)\n",
    "    \n",
    "    # ATR calculation\n",
    "    df['ATR'] = df['TR'].rolling(period).mean()\n",
    "    \n",
    "    # Drop the TR column if not needed\n",
    "    df.drop('TR', axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rice Rate of Change (ROC)\n",
    "def calculate_roc(df, n_periods=12, column='Adj Close'):\n",
    "    \"\"\"\n",
    "    Calculate the Price Rate of Change (ROC) of a stock dataset.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing stock data, with 'Adj Close' column representing closing prices.\n",
    "        n_periods (int): Number of periods for which to calculate the ROC. # It can be anything such as 12, 25,\n",
    "        or 200. Short-term trader traders typically use a smaller number while longer-term investors use a larger\n",
    "        number.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with 'ROC' column containing the calculated ROC values.\n",
    "    \"\"\"\n",
    "    close_prices = df[column]\n",
    "    close_prices_shifted = close_prices.shift(n_periods)\n",
    "    \n",
    "    roc = ((close_prices - close_prices_shifted) / close_prices_shifted) * 100\n",
    "    \n",
    "    df['ROC'] = roc\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Money Flow Index - MFI\n",
    "def calculate_mfi(df, period=14):\n",
    "    \"\"\"\n",
    "    Calculate the Money Flow Index (MFI) of a stock dataset.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing stock data, with 'High', 'Low', 'Close', and 'Volume' columns representing high, low, closing prices, and volume respectively.\n",
    "        period (int): Number of periods for which to calculate the MFI (default is 14).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with 'MFI' column containing the calculated MFI values.\n",
    "    \"\"\"\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "    close = df['Adj Close']\n",
    "    volume = df['Volume']\n",
    "    \n",
    "    # Typical Price calculation\n",
    "    tp = (high + low + close) / 3\n",
    "    \n",
    "    # Raw Money Flow calculation\n",
    "    mf = tp * volume\n",
    "    \n",
    "    # Determine whether the typical price is higher or lower than the previous period\n",
    "    tp_shifted = tp.shift(1)\n",
    "    positive_flow = (tp > tp_shifted)\n",
    "    negative_flow = (tp < tp_shifted)\n",
    "    \n",
    "    # Calculate positive and negative money flow\n",
    "    positive_mf = positive_flow * mf\n",
    "    negative_mf = negative_flow * mf\n",
    "    \n",
    "    # Calculate the Money Flow Ratio (MFR)\n",
    "    mfr = positive_mf.rolling(window=period).sum() / negative_mf.rolling(window=period).sum()\n",
    "    \n",
    "    # Calculate the Money Flow Index (MFI)\n",
    "    mfi = 100 - (100 / (1 + mfr))\n",
    "    \n",
    "    df['MFI'] = mfi\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chaikin_oscillator\n",
    "def calculate_chaikin_oscillator(df, short_period=3, long_period=10):\n",
    "    \"\"\"\n",
    "    Calculate the Chaikin Oscillator of a stock dataset.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing stock data, with 'High', 'Low', 'Adj Close', and 'Volume' columns representing high, low, closing prices, and volume respectively.\n",
    "        short_period (int): Number of periods for the short EMA (default is 3).\n",
    "        long_period (int): Number of periods for the long EMA (default is 10).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with 'Chaikin_Oscillator' column containing the calculated Chaikin Oscillator values.\n",
    "    \"\"\"\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "    close = df['Adj Close']\n",
    "    volume = df['Volume']\n",
    "    \n",
    "    # Money Flow Multiplier calculation\n",
    "    mfm = ((close - low) - (high - close)) / (high - low)\n",
    "    \n",
    "    # Money Flow Volume calculation\n",
    "    mfv = mfm * volume\n",
    "    \n",
    "    # Accumulation/Distribution Line (ADL) calculation\n",
    "    adl = mfv.cumsum()\n",
    "    \n",
    "    # Calculate the EMA for ADL\n",
    "    ema_short = adl.ewm(span=short_period, min_periods=short_period, adjust=False).mean()\n",
    "    ema_long = adl.ewm(span=long_period, min_periods=long_period, adjust=False).mean()\n",
    "    \n",
    "    # Calculate the Chaikin Oscillator\n",
    "    chaikin_oscillator = ema_short - ema_long\n",
    "    \n",
    "    df['Chaikin_Oscillator'] = chaikin_oscillator\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bulid the technical indicators: features\n",
    "def technical_indicators(df):\n",
    "    df = calculate_sma(df)\n",
    "    df = calculate_wma(df)\n",
    "    df = calculate_macd(df)\n",
    "    df = calculate_rsi(df)\n",
    "    df = calculate_stochastic_oscillator(df)\n",
    "    df = calculate_bollinger_bands(df)\n",
    "    df = calculate_williams_percent_r(df)\n",
    "    df = calculate_obv(df)\n",
    "    df = calculate_roc(df)\n",
    "    df = calculate_atr(df)\n",
    "    df = calculate_mfi(df)\n",
    "    df = calculate_chaikin_oscillator(df)\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All analyzed technical indicators are lagged by one period before being used as predictors for returns in the models in order to avoid the so-called look ahead \n",
    "# bias involving making decisions in the same period for which the given signal was generated.\n",
    "def lag_technical_indicators(df):\n",
    "    \"\"\"\n",
    "    Lag all columns in a DataFrame by one period.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the calculated technical indicators.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with all columns lagged by one period.\n",
    "    \"\"\"\n",
    "    # Lag all columns by one period\n",
    "    df_lagged = df.shift()\n",
    "    \n",
    "    return df_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation Function\n",
    "# Technical analysis indicators need to be rescaled before being fed to the models.\n",
    "# The process is conducted using a version of min-max normalization technique which produces outputs in range from ‐1 to 1.\n",
    "# This technique was chosen for two reasons: it is intuitive as the machine learning models produce output \n",
    "# variable that is also ranging from ‐1 to 1 and because it causes the input data to be more comparable. \n",
    "# X'(t) = (X(t) - min(x)) / (max(x) - min(x))*2 -1\n",
    "\n",
    "def feature_transform(df):\n",
    "    \"\"\"\n",
    "    Transform all columns in the DataFrame as the following formula\n",
    "    X'(t) = (X(t) - min(x)) / (max(x) - min(x))*2 -1\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the calculated technical indicators.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with all columns transformed.\n",
    "    \"\"\"\n",
    "    max_x = df.max()\n",
    "    min_x = df.min()\n",
    "\n",
    "    df_transformed = (df - min_x) / (max_x - min_x) * 2 - 1\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#truncate the dataframe from the biggining so the walk forward splits will continue untill the last date\n",
    "def truncate_before_wf(df, in_sample_size, out_sample_size):\n",
    "    drop_index = (len(df) - in_sample_size) % out_sample_size\n",
    "    return (df.iloc[drop_index:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research employed dynamic estimation windows which means that the underlying parameters of the models \n",
    "# were periodically recalibrated to reflect current market behaviors. Observations from the beginning \n",
    "# of the available period were initially trimmed in order for the overall number of observations for\n",
    "# each index to be easily divisible into equal subsets. Calibration of models’ parameters was conducted \n",
    "# on 200 trading day window (in-sample) and then model predictions were applied onto next 20 trading day\n",
    "# window (out-of-sample). For each subsequent dynamic window iteration, in-sample and out-of-sample moved \n",
    "# by 20 trading days. \n",
    "\n",
    "def walk_forward_validation(df, in_sample_size, out_sample_size):\n",
    "    \"\"\"\n",
    "    Perform walk-forward validation on a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "        in_sample_size (int): Number of periods to use for in-sample data.\n",
    "        out_sample_size (int): Number of periods to use for out-of-sample data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Tuple containing lists of in-sample and out-of-sample data.\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    n_subsets = (total_rows - in_sample_size) // out_sample_size\n",
    "    splits = []\n",
    "        \n",
    "    for i in range(n_subsets):\n",
    "        start_index = i * out_sample_size\n",
    "        end_index = start_index + in_sample_size + out_sample_size\n",
    "        \n",
    "        if end_index > total_rows:\n",
    "            break\n",
    "        \n",
    "        in_sample = df.iloc[start_index : start_index + in_sample_size]\n",
    "        out_of_sample = df.iloc[start_index + in_sample_size : end_index]\n",
    "        \n",
    "        splits.append((in_sample, out_of_sample))\n",
    "    return (splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation coefficients between each feature and the return & print it\n",
    "\n",
    "def correlation(df, target_name):\n",
    "\n",
    "    correlation_with_target = np.abs(df.corrwith(df[target_name]))\n",
    "\n",
    "    # Display the correlation coefficients\n",
    "    print(\"Correlation with Log return:\")\n",
    "    print(correlation_with_target.sort_values(ascending=False))\n",
    "    correlation_with_target.sort_values().plot.barh(color = 'blue',title = 'Strength of Correlation', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thus far, we've only used a simple correlation statistic across the full time period. \n",
    "#This is a good place to start but, is a dangerous place to stop. Financial time series data suffers\n",
    "# from non-stationarity and regime change, so a relationship which on average has existed may have been \n",
    "#wildly unstable over time.\n",
    "\n",
    "#To check, we'll plot the rolling correlation of these selected features.\n",
    "\n",
    "# Compute the rolling correlation for each pair of selected features\n",
    "def rolling_correlation(df, target_name, window_size = 200):\n",
    "\n",
    "    correlation_with_target_200 = df.rolling(window=window_size).corr(df[target_name])\n",
    "    # Create traces for each feature\n",
    "    traces = []\n",
    "    for feature in df.columns:\n",
    "        trace = go.Scatter(\n",
    "            x=correlation_with_target_200.index,\n",
    "            y=correlation_with_target_200[feature],\n",
    "            mode='lines',\n",
    "            name=feature\n",
    "        )\n",
    "        traces.append(trace)\n",
    "\n",
    "    # Create layout for the plot\n",
    "        layout = go.Layout(\n",
    "        title='Rolling Correlation of Features with Log Return',\n",
    "        xaxis=dict(title='Index'),\n",
    "        yaxis=dict(title='Rolling Correlation with Log Return'),\n",
    "        hovermode='closest',\n",
    "        autosize=True\n",
    "    )\n",
    "\n",
    "    # Create figure object\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "\n",
    "    # Show plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pair plot\n",
    "def features_paiplot(df):\n",
    "    pairplot = sns.pairplot(df, height=1.5)\n",
    "\n",
    "    # Set the title\n",
    "    pairplot.figure.suptitle('Pairplot of features', y=1.02)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_outliers_iqr_df(df, k=1.5):\n",
    "    \"\"\"\n",
    "    Count the number of outliers in each column of the DataFrame using the Interquartile Range (IQR) method.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The input DataFrame.\n",
    "    - k: The multiplier for the IQR. Typically set to 1.5 to 3.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary where keys are column names and values are the number of outliers detected in each column.\n",
    "    \"\"\"\n",
    "    outliers_counts = {}\n",
    "    for col in df.columns:\n",
    "        data = df[col]\n",
    "        quartile_1, quartile_3 = np.percentile(data, [25, 75])\n",
    "        iqr = quartile_3 - quartile_1\n",
    "        lower_bound = quartile_1 - (k * iqr)\n",
    "        upper_bound = quartile_3 + (k * iqr)\n",
    "        outliers = (data < lower_bound) | (data > upper_bound)\n",
    "        outliers_counts[col] = np.sum(outliers)\n",
    "    return outliers_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Truncate NaN Data\n",
    "def drop_nan(df):\n",
    "    # Remove rows with NaN values\n",
    "    cleaned_df = df.dropna()\n",
    "\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check distribution \n",
    "def check_distribution(df, column_name='Adj Close'):\n",
    "    \"\"\"\n",
    "    Check the distribution of a column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "        column_name (str): Name of the column to check the distribution for (default is 'Adj Close').\n",
    "\n",
    "    Returns:\n",
    "        None (displays descriptive statistics and visualizations)\n",
    "    \"\"\"\n",
    "    # Descriptive statistics\n",
    "    print(\"Descriptive Statistics:\")\n",
    "    print(df[column_name].describe())\n",
    "\n",
    "    # Histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df[column_name], kde=True)\n",
    "    plt.title(f'Distribution of {column_name}')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_normal(df, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Test if the data is normally distributed using Z-score.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The input data array.\n",
    "    - alpha: The significance level for the test.\n",
    "    \n",
    "    Returns:\n",
    "    - True if the data is normally distributed, False otherwise.\n",
    "    \"\"\"\n",
    "    normal_col = {}\n",
    "    for col in df.columns:\n",
    "        data = df[col]\n",
    "        z_score, p_value = stats.normaltest(data)\n",
    "        normal_col[col] = p_value > alpha\n",
    "    return normal_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Percentage Error (MAPE)\n",
    "    \n",
    "    Args:\n",
    "    actual: array-like, actual values\n",
    "    predicted: array-like, predicted values\n",
    "    \n",
    "    Returns:\n",
    "    mape: float, MAPE value\n",
    "    \"\"\"\n",
    "    # Ensure both actual and predicted arrays have the same length\n",
    "    if len(actual) != len(predicted):\n",
    "        raise ValueError(\"Length of actual and predicted arrays must be the same.\")\n",
    "    \n",
    "\n",
    "    # Calculate absolute percentage error for each observation\n",
    "    abs_percentage_error = np.abs((actual - predicted) / np.maximum(np.abs(actual), 1e-10))\n",
    "    \n",
    "    # Calculate mean of absolute percentage errors\n",
    "    mape = np.mean(abs_percentage_error) * 100\n",
    "    \n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sharpe_ratio(df, column, risk_free_rate=0.0, annualized=True):\n",
    "    \"\"\"\n",
    "    Calculate the Sharpe Ratio for the S&P 500 index.\n",
    "\n",
    "    Parameters:\n",
    "    - sp500_df: DataFrame containing historical S&P 500 index data with a 'Close' column.\n",
    "    - risk_free_rate: Annualized risk-free rate of return (default is 0.0).\n",
    "    - annualized: If True, return annualized Sharpe Ratio; if False, return non-annualized Sharpe Ratio (default is True).\n",
    "\n",
    "    Returns:\n",
    "    - sharpe_ratio: The calculated Sharpe Ratio.\n",
    "    \"\"\"\n",
    "    # Calculate excess returns (returns above risk-free rate)\n",
    "    excess_returns = df[column] - risk_free_rate / 252  # Assuming 252 trading days in a year\n",
    "\n",
    "    # Calculate Sharpe Ratio\n",
    "    sharpe_ratio = excess_returns.mean() / excess_returns.std()\n",
    "\n",
    "    # Annualize Sharpe Ratio if required\n",
    "    if annualized:\n",
    "        sharpe_ratio *= (252 ** 0.5)\n",
    "\n",
    "    return sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ir(stock_returns, benchmark_returns):\n",
    "    # Calculate excess returns\n",
    "    excess_returns = stock_returns - benchmark_returns\n",
    "    \n",
    "    # Calculate average excess return\n",
    "    avg_excess_return = np.mean(excess_returns)\n",
    "    \n",
    "    # Calculate standard deviation of excess returns\n",
    "    std_excess_return = np.std(excess_returns)\n",
    "    \n",
    "    # Calculate Information Ratio\n",
    "    ir = avg_excess_return / std_excess_return\n",
    "    \n",
    "    return ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mdd(returns):\n",
    "    # Calculate cumulative returns\n",
    "    cum_returns = (1 + returns).cumprod()\n",
    "    \n",
    "    # Calculate the maximum value seen up to each point\n",
    "    max_seen = cum_returns.cummax()\n",
    "    \n",
    "    # Calculate drawdowns\n",
    "    drawdowns = (cum_returns - max_seen) / max_seen\n",
    "    \n",
    "    # Find the maximum drawdown\n",
    "    max_drawdown = drawdowns.min()\n",
    "    \n",
    "    return max_drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_series, predicted_series):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of a model that predicts buy, hold, or sell signals.\n",
    "\n",
    "    Parameters:\n",
    "    true_series (Series): Pandas Series containing true labels.\n",
    "    predicted_series (Series): Pandas Series containing predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    accuracy (float): Accuracy of the model.\n",
    "    \"\"\"\n",
    "    if len(true_series) != len(predicted_series):\n",
    "        raise ValueError(\"The lengths of true_series and predicted_series must be equal.\")\n",
    "\n",
    "    correct_predictions = sum(1 for true_label, predicted_label in zip(true_series,\n",
    "                             predicted_series) if true_label == predicted_label)\n",
    "    total_predictions = len(true_series)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(df, column_to_diff='Predictions', column='Predicted Returns'):\n",
    "    \"\"\"\n",
    "    Calculate returns from adjusted close prices.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing adjusted close prices.\n",
    "\n",
    "    Returns:\n",
    "    returns (DataFrame): DataFrame containing the calculated returns in df[column].\n",
    "    \"\"\"\n",
    "    \n",
    "    df[column] = df[column_to_diff].pct_change()\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quantiles(df, column='Predected Return', q1=0.25, q2=0.5, q3=0.75):\n",
    "    Q1 = df[column].quantile(q1)\n",
    "    Q2 = df[column].quantile(q2)  # Median\n",
    "    Q3 = df[column].quantile(q3)\n",
    "\n",
    "    return (Q1, Q2, Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_signal(df, return_column='Predicted Returns', signal_column='Predicted Signal', q1=None, q3=None):\n",
    "    if q1 is None or q3 is None:\n",
    "        q1 = df[return_column].quantile(0.25)\n",
    "        q3 = df[return_column].quantile(0.75)\n",
    "\n",
    "    df[signal_column] = 0  # Default signal\n",
    "\n",
    "    df.loc[df[return_column] >= q3, signal_column] = 1\n",
    "    df.loc[df[return_column] <= q1, signal_column] = -1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy and add features\n",
    "sp500 = sp500_data.copy()\n",
    "sp500 = technical_indicators(sp500)\n",
    "sp500_val = sp500_validation_set.copy()\n",
    "sp500_val = technical_indicators(sp500_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laging technical indicators to avoid look_ahead bias\n",
    "#sp500.iloc[:, 6:] = sp500.iloc[:, 6:].apply(lag_technical_indicators, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nagham\\AppData\\Local\\Temp\\ipykernel_24024\\3326783675.py:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "2002-01-02   -0.934138\n",
      "2002-01-03   -0.932189\n",
      "2002-01-04   -0.930082\n",
      "2002-01-07   -0.931904\n",
      "2002-01-08   -0.933658\n",
      "                ...   \n",
      "2022-12-23    0.780322\n",
      "2022-12-27    0.776101\n",
      "2022-12-28    0.771805\n",
      "2022-12-29    0.775990\n",
      "2022-12-30    0.771838\n",
      "Name: OBV, Length: 5287, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  sp500.iloc[:, 6:] = sp500.iloc[:, 6:].apply(feature_transform, axis=0)\n"
     ]
    }
   ],
   "source": [
    "#transform features\n",
    "sp500.iloc[:, 6:] = sp500.iloc[:, 6:].apply(feature_transform, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation before choosing features\n",
    "#correlation(sp500, 'Adj Close')\n",
    "#rolling_correlation(sp500, 'Adj Close')\n",
    "\n",
    "# Step 1: Take the most strongly correlated feature and add it to our list of selected features. \n",
    "# Step 2: Take the second correlated feature and check to see if it's closely correlated \n",
    "# (neighboring in the clustermap) to any features already chosen.\n",
    "# If no, add to the list. If yes, discard. \n",
    "# Step 3: Repeat this process until either (1) we've reached the target feature count,\n",
    "# or (2) we've run out strongly correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data from na values & pick the columns\n",
    "sp500_d = sp500.copy()\n",
    "sp500_d = drop_nan(sp500_d)\n",
    "sp500_d = sp500_d.loc[:, ['SMA','WMA', 'MACD', 'RSI', '%K_fast', '%D_fast', '%D_slow', 'Bollinger Diff',\n",
    "                    'WPR', 'OBV', 'ROC', 'ATR', 'MFI', 'Chaikin_Oscillator', 'Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics:\n",
      "count    5268.000000\n",
      "mean     1929.782262\n",
      "std       993.618063\n",
      "min       676.530029\n",
      "25%      1184.485016\n",
      "50%      1475.195007\n",
      "75%      2496.524963\n",
      "max      4796.560059\n",
      "Name: Adj Close, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_distribution(sp500_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the correlated data:\n",
    "#features_paiplot(sp500_d)\n",
    "#correlation(sp500_d, 'Adj Close')\n",
    "#rolling_correlation(sp500, 'Adj Close')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is data normal? {'SMA': False, 'WMA': False, 'MACD': False, 'RSI': False, '%K_fast': False, '%D_fast': False, '%D_slow': False, 'Bollinger Diff': False, 'WPR': False, 'OBV': False, 'ROC': False, 'ATR': False, 'MFI': False, 'Chaikin_Oscillator': False, 'Adj Close': False}\n"
     ]
    }
   ],
   "source": [
    "print(\"Is data normal?\", is_normal(sp500_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers in each column:\n",
      "{'SMA': 106, 'WMA': 110, 'MACD': 410, 'RSI': 2, '%K_fast': 0, '%D_fast': 0, '%D_slow': 0, 'Bollinger Diff': 466, 'WPR': 0, 'OBV': 0, 'ROC': 262, 'ATR': 514, 'MFI': 8, 'Chaikin_Oscillator': 64, 'Adj Close': 109}\n"
     ]
    }
   ],
   "source": [
    "outliers_counts = count_outliers_iqr_df(sp500_d)\n",
    "print(\"Number of outliers in each column:\")\n",
    "print(outliers_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing features:\n",
    "chosen_features = ['SMA','WMA', 'MACD', 'RSI', '%K_fast', '%D_fast', '%D_slow', 'Bollinger Diff',\n",
    "                    'WPR', 'OBV', 'ROC', 'ATR', 'MFI', 'Chaikin_Oscillator', 'Adj Close']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the context of regression tasks like this one (since SVR is a Support Vector Machine for regression),\n",
    "# where the target variable is continuous (e.g., log returns), a model score less than 0 typically indicates \n",
    "# that the model is performing poorly and making predictions that are worse than simply using the mean or another\n",
    "# basic statistical measure as the prediction for all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arima Model without Time Sieries Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data from na values & pick the columns\n",
    "data = sp500.copy()\n",
    "data = drop_nan(data)\n",
    "data = data.loc[:, ['SMA','WMA', 'MACD', 'RSI', '%K_fast', '%D_fast', '%D_slow', 'Bollinger Diff',\n",
    "                    'WPR', 'OBV', 'ROC', 'ATR', 'MFI', 'Chaikin_Oscillator', 'Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=48816.654, Time=4.65 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=49009.109, Time=0.08 sec\n",
      " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=48949.510, Time=0.20 sec\n",
      " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=48953.938, Time=0.57 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0]             : AIC=49009.302, Time=0.06 sec\n",
      " ARIMA(1,1,2)(0,0,0)[0] intercept   : AIC=48946.873, Time=1.01 sec\n",
      " ARIMA(2,1,1)(0,0,0)[0] intercept   : AIC=48947.314, Time=0.97 sec\n",
      " ARIMA(3,1,2)(0,0,0)[0] intercept   : AIC=48818.306, Time=4.70 sec\n",
      " ARIMA(2,1,3)(0,0,0)[0] intercept   : AIC=48935.067, Time=6.24 sec\n",
      " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=48946.502, Time=1.03 sec\n",
      " ARIMA(1,1,3)(0,0,0)[0] intercept   : AIC=48943.647, Time=3.38 sec\n",
      " ARIMA(3,1,1)(0,0,0)[0] intercept   : AIC=48945.907, Time=2.91 sec\n",
      " ARIMA(3,1,3)(0,0,0)[0] intercept   : AIC=48815.978, Time=7.76 sec\n",
      " ARIMA(4,1,3)(0,0,0)[0] intercept   : AIC=48817.966, Time=8.61 sec\n",
      " ARIMA(3,1,4)(0,0,0)[0] intercept   : AIC=48819.889, Time=9.06 sec\n",
      " ARIMA(2,1,4)(0,0,0)[0] intercept   : AIC=48823.600, Time=9.74 sec\n",
      " ARIMA(4,1,2)(0,0,0)[0] intercept   : AIC=48827.906, Time=8.17 sec\n",
      " ARIMA(4,1,4)(0,0,0)[0] intercept   : AIC=48799.514, Time=9.86 sec\n",
      " ARIMA(5,1,4)(0,0,0)[0] intercept   : AIC=48818.000, Time=10.89 sec\n",
      " ARIMA(4,1,5)(0,0,0)[0] intercept   : AIC=48804.248, Time=11.64 sec\n",
      " ARIMA(3,1,5)(0,0,0)[0] intercept   : AIC=48821.678, Time=6.77 sec\n",
      " ARIMA(5,1,3)(0,0,0)[0] intercept   : AIC=48878.224, Time=8.25 sec\n",
      " ARIMA(5,1,5)(0,0,0)[0] intercept   : AIC=48806.168, Time=11.12 sec\n",
      " ARIMA(4,1,4)(0,0,0)[0]             : AIC=48800.136, Time=4.59 sec\n",
      "\n",
      "Best model:  ARIMA(4,1,4)(0,0,0)[0] intercept\n",
      "Total fit time: 132.285 seconds\n",
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                 5268\n",
      "Model:               SARIMAX(4, 1, 4)   Log Likelihood              -24389.757\n",
      "Date:                Wed, 17 Apr 2024   AIC                          48799.514\n",
      "Time:                        09:08:01   BIC                          48865.206\n",
      "Sample:                             0   HQIC                         48822.477\n",
      "                               - 5268                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept      0.6627      0.566      1.171      0.241      -0.446       1.772\n",
      "ar.L1         -0.2495      0.013    -18.623      0.000      -0.276      -0.223\n",
      "ar.L2          0.7760      0.014     55.980      0.000       0.749       0.803\n",
      "ar.L3         -0.3714      0.012    -30.186      0.000      -0.395      -0.347\n",
      "ar.L4         -0.8491      0.013    -65.346      0.000      -0.875      -0.824\n",
      "ma.L1          0.1842      0.016     11.433      0.000       0.153       0.216\n",
      "ma.L2         -0.7506      0.017    -44.727      0.000      -0.783      -0.718\n",
      "ma.L3          0.4183      0.014     29.337      0.000       0.390       0.446\n",
      "ma.L4          0.7448      0.016     46.065      0.000       0.713       0.777\n",
      "sigma2       605.2320      4.575    132.302      0.000     596.266     614.198\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   1.89   Jarque-Bera (JB):             45853.46\n",
      "Prob(Q):                              0.17   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               6.49   Skew:                            -0.96\n",
      "Prob(H) (two-sided):                  0.00   Kurtosis:                        17.33\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n"
     ]
    }
   ],
   "source": [
    "# Auto ARIMA to select optimal ARIMA parameters\n",
    "model = auto_arima(data['Adj Close'], seasonal=False, trace=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Arima Model is ARIMA(2,1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data Splitted 80% - 20% - Walk Forward on test (the predictions added to the train)\n",
    "The Method of WF as it was used for other Models WAS NOT USED HERE!!!\n",
    "The Predictions were really bad when it was tried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "c:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n",
      "c:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "LU decomposition error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test)):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Generate a prediction\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     yhat \u001b[38;5;241m=\u001b[39m \u001b[43marima_forecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(yhat)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Add the predicted value to the training set\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[50], line 6\u001b[0m, in \u001b[0;36marima_forecast\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marima_forecast\u001b[39m(history):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     model \u001b[38;5;241m=\u001b[39m ARIMA(history, order\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m     model_fit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Make the prediction\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     output \u001b[38;5;241m=\u001b[39m model_fit\u001b[38;5;241m.\u001b[39mforecast()\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\statsmodels\\tsa\\arima\\model.py:395\u001b[0m, in \u001b[0;36mARIMA.fit\u001b[1;34m(self, start_params, transformed, includes_fixed, method, method_kwargs, gls, gls_kwargs, cov_type, cov_kwds, return_params, low_memory)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m     method_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 395\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcov_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcov_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcov_kwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethod_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_params:\n\u001b[0;32m    399\u001b[0m         res\u001b[38;5;241m.\u001b[39mfit_details \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mmlefit\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:704\u001b[0m, in \u001b[0;36mMLEModel.fit\u001b[1;34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m         flags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhessian_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m optim_hessian\n\u001b[0;32m    703\u001b[0m     fargs \u001b[38;5;241m=\u001b[39m (flags,)\n\u001b[1;32m--> 704\u001b[0m     mlefit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMLEModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mfargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mskip_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;66;03m# Just return the fitted parameters if requested\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_params:\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\statsmodels\\base\\model.py:566\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_t\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    565\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Optimizer()\n\u001b[1;32m--> 566\u001b[0m xopt, retvals, optim_settings \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mhessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;66;03m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[0;32m    576\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:242\u001b[0m, in \u001b[0;36mOptimizer._fit\u001b[1;34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[0m\n\u001b[0;32m    239\u001b[0m     fit_funcs\u001b[38;5;241m.\u001b[39mupdate(extra_fit_funcs)\n\u001b[0;32m    241\u001b[0m func \u001b[38;5;241m=\u001b[39m fit_funcs[method]\n\u001b[1;32m--> 242\u001b[0m xopt, retvals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mhess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhessian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m optim_settings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: method, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_params\u001b[39m\u001b[38;5;124m'\u001b[39m: start_params,\n\u001b[0;32m    248\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: maxiter, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_output\u001b[39m\u001b[38;5;124m'\u001b[39m: full_output,\n\u001b[0;32m    249\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfargs\u001b[39m\u001b[38;5;124m'\u001b[39m: fargs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[0;32m    250\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretall\u001b[39m\u001b[38;5;124m'\u001b[39m: retall, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_fit_funcs\u001b[39m\u001b[38;5;124m\"\u001b[39m: extra_fit_funcs}\n\u001b[0;32m    251\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:659\u001b[0m, in \u001b[0;36m_fit_lbfgs\u001b[1;34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m approx_grad:\n\u001b[0;32m    657\u001b[0m     func \u001b[38;5;241m=\u001b[39m f\n\u001b[1;32m--> 659\u001b[0m retvals \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin_l_bfgs_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[0;32m    665\u001b[0m     xopt, fopt, d \u001b[38;5;241m=\u001b[39m retvals\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:199\u001b[0m, in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[0;32m    187\u001b[0m callback \u001b[38;5;241m=\u001b[39m _wrap_callback(callback)\n\u001b[0;32m    188\u001b[0m opts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp,\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m'\u001b[39m: iprint,\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxcor\u001b[39m\u001b[38;5;124m'\u001b[39m: m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m'\u001b[39m: maxls}\n\u001b[1;32m--> 199\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m d \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjac\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    202\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    203\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuncalls\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfev\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    204\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    205\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarnflag\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m    206\u001b[0m f \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:369\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    363\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    372\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:296\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\statsmodels\\base\\model.py:534\u001b[0m, in \u001b[0;36mLikelihoodModel.fit.<locals>.f\u001b[1;34m(params, *args)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(params, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m--> 534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m nobs\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:939\u001b[0m, in \u001b[0;36mMLEModel.loglike\u001b[1;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m complex_step:\n\u001b[0;32m    937\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minversion_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m INVERT_UNIVARIATE \u001b[38;5;241m|\u001b[39m SOLVE_LU\n\u001b[1;32m--> 939\u001b[0m loglike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;66;03m# Koopman, Shephard, and Doornik recommend maximizing the average\u001b[39;00m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;66;03m# likelihood to avoid scale issues, but the averaging is done\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;66;03m# automatically in the base model `fit` method\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loglike\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\kalman_filter.py:1001\u001b[0m, in \u001b[0;36mKalmanFilter.loglike\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;124;03mCalculate the loglikelihood associated with the statespace model.\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;124;03m    The joint loglikelihood.\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    999\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconserve_memory\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1000\u001b[0m                   MEMORY_CONSERVE \u001b[38;5;241m^\u001b[39m MEMORY_NO_LIKELIHOOD)\n\u001b[1;32m-> 1001\u001b[0m kfilter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m loglikelihood_burn \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloglikelihood_burn\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1003\u001b[0m                                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloglikelihood_burn)\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconserve_memory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m&\u001b[39m MEMORY_NO_LIKELIHOOD):\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\kalman_filter.py:921\u001b[0m, in \u001b[0;36mKalmanFilter._filter\u001b[1;34m(self, filter_method, inversion_method, stability_method, conserve_memory, filter_timing, tolerance, loglikelihood_burn, complex_step)\u001b[0m\n\u001b[0;32m    918\u001b[0m kfilter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kalman_filters[prefix]\n\u001b[0;32m    920\u001b[0m \u001b[38;5;66;03m# Initialize the state\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Run the filter\u001b[39;00m\n\u001b[0;32m    924\u001b[0m kfilter()\n",
      "File \u001b[1;32mc:\\Users\\Nagham\\MambaPython\\envs\\TechnionAiProg\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\representation.py:1058\u001b[0m, in \u001b[0;36mRepresentation._initialize_state\u001b[1;34m(self, prefix, complex_step)\u001b[0m\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialization\u001b[38;5;241m.\u001b[39minitialized:\n\u001b[0;32m   1057\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitialization is incomplete.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_statespaces\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatespace model not initialized.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mstatsmodels/tsa/statespace/_representation.pyx:1373\u001b[0m, in \u001b[0;36mstatsmodels.tsa.statespace._representation.dStatespace.initialize\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstatsmodels/tsa/statespace/_representation.pyx:1362\u001b[0m, in \u001b[0;36mstatsmodels.tsa.statespace._representation.dStatespace.initialize\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstatsmodels/tsa/statespace/_initialization.pyx:288\u001b[0m, in \u001b[0;36mstatsmodels.tsa.statespace._initialization.dInitialization.initialize\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstatsmodels/tsa/statespace/_initialization.pyx:406\u001b[0m, in \u001b[0;36mstatsmodels.tsa.statespace._initialization.dInitialization.initialize_stationary_stationary_cov\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstatsmodels/tsa/statespace/_tools.pyx:1548\u001b[0m, in \u001b[0;36mstatsmodels.tsa.statespace._tools._dsolve_discrete_lyapunov\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: LU decomposition error."
     ]
    }
   ],
   "source": [
    "dataset_ex_df = data['Adj Close']\n",
    "# Define the ARIMA model\n",
    "def arima_forecast(history):\n",
    "    # Fit the model\n",
    "    model = ARIMA(history, order=(2,1,2))\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    # Make the prediction\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    return yhat\n",
    "\n",
    "# Split data into train and test sets\n",
    "X = dataset_ex_df.values\n",
    "size = int(len(X) * 0.8)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "\n",
    "# Walk-forward validation\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "for t in range(len(test)):\n",
    "    # Generate a prediction\n",
    "    yhat = arima_forecast(history)\n",
    "    predictions.append(yhat)\n",
    "    # Add the predicted value to the training set\n",
    "    obs = test[t]\n",
    "    history.append(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arima Train set Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate\n",
    "mae = mean_absolute_error(test,predictions)\n",
    "mse = mean_squared_error(test,predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(test,predictions)\n",
    "mape = mean_absolute_percentage_error(test,predictions)\n",
    "\n",
    "# Visualize predictions vs. actual values\n",
    "plt.scatter(test,predictions)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values\")\n",
    "plt.show()\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}\\nMean Squared Error: {mse}\\nRoot Mean Squared Error: {rmse}\\nR-squared: {r2}')\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "# Create traces\n",
    "trace1 = go.Scatter(x=dataset_ex_df.iloc[size:].index, y=test, mode='lines', name='Adjusted Close')\n",
    "trace2 = go.Scatter(x=dataset_ex_df.iloc[size:].index, y=predictions, mode='lines', name='Adj_Close_diff')\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces to figure\n",
    "fig.add_trace(trace1)\n",
    "fig.add_trace(trace2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='ARIMA Predictions vs Actual Values',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='USD',\n",
    "                  hovermode='x unified')\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arima Validation set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_val = sp500_validation_set.copy()\n",
    "sp500_val = technical_indicators(sp500_val)\n",
    "\n",
    "#predict on the model\n",
    "    # Make the prediction\n",
    "output = model_fit.forecast()\n",
    "yhat = output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate\n",
    "mae = mean_absolute_error(test,predictions)\n",
    "mse = mean_squared_error(test,predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(test,predictions)\n",
    "mape = mean_absolute_percentage_error(test,predictions)\n",
    "# Visualize predictions vs. actual values\n",
    "plt.scatter(test,predictions)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values\")\n",
    "plt.show()\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}\\nMean Squared Error: {mse}\\nRoot Mean Squared Error: {rmse}\\nR-squared: {r2}')\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "# Create traces\n",
    "trace1 = go.Scatter(x=dataset_ex_df.iloc[size:].index, y=test, mode='lines', name='Adjusted Close')\n",
    "trace2 = go.Scatter(x=dataset_ex_df.iloc[size:].index, y=predictions, mode='lines', name='Adj_Close_diff')\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces to figure\n",
    "fig.add_trace(trace1)\n",
    "fig.add_trace(trace2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='ARIMA Predictions vs Actual Values',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='USD',\n",
    "                  hovermode='x unified')\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the results of WF validation!! Results were BAD!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data from na values & pick the columns\n",
    "data = sp500.copy()\n",
    "data = drop_nan(data)\n",
    "data = data.loc[:, ['SMA','WMA', 'MACD', 'RSI', '%K_fast', '%D_fast', '%D_slow', 'Bollinger Diff',\n",
    "                    'WPR', 'OBV', 'ROC', 'ATR', 'MFI', 'Chaikin_Oscillator', 'Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto ARIMA to select optimal ARIMA parameters\n",
    "model = auto_arima(data['Adj Close'], seasonal=False, trace=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "data = truncate_before_wf(data, 200, 20)\n",
    "splits_arima = walk_forward_validation(data, 200, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = (2, 1, 2)  # From Auto test\n",
    "data['Predictions_Arima'] = np.nan\n",
    "arima_results_list = []\n",
    "all_predictions = []\n",
    "\n",
    "for in_sample, out_of_sample in splits_arima:\n",
    "    arima_model = ARIMA(in_sample['Adj Close'], order=order)\n",
    "    arima_results = arima_model.fit()\n",
    "    forecast = arima_results.forecast(steps=len(out_of_sample['Adj Close']))\n",
    "    arima_results_list.append((arima_results, forecast))\n",
    "    all_predictions.append(forecast)\n",
    "\n",
    "# Concatenate all predictions into a single DataFrame\n",
    "all_predictions_df = pd.DataFrame(np.concatenate(all_predictions), columns=['Predictions_Arima'])\n",
    "\n",
    "# Update the 'Predictions_Arima' column in the original DataFrame\n",
    "data.loc[data.index[200:], 'Predictions_Arima'] = all_predictions_df['Predictions_Arima'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop Nan values from the first 200 walkforward samples(no predictions)\n",
    "data = drop_nan(data)\n",
    "\n",
    "# Visual Inspection: Actual values against the predicted values \n",
    "feature1 = 'Adj Close'\n",
    "feature2 = 'Predictions_Arima'\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=data.index,  # Assuming the index represents x-axis values\n",
    "    y=data[feature1],\n",
    "    mode='lines',\n",
    "    name=feature1\n",
    "))\n",
    "\n",
    "# Add another line plot for the second feature\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.index,  # Assuming the index represents x-axis values\n",
    "    y=data[feature2],\n",
    "    mode='lines',\n",
    "    name=feature2\n",
    "))\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Arima (2,1,2) - Adj Close Vs. Predictions on train dataset - WF 200,20',\n",
    "    xaxis=dict(title='X Axis'),  # Customize x-axis label\n",
    "    yaxis=dict(title='Y Axis'),  # Customize y-axis label\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate\n",
    "\n",
    "mae = mean_absolute_error(data['Adj Close'],data['Predictions_Arima'])\n",
    "mse = mean_squared_error(data['Adj Close'],data['Predictions_Arima'])\n",
    "rmse = np.sqrt(mse)\n",
    "mape = calculate_mape(data['Adj Close'], data['Predictions_Arima'])\n",
    "\n",
    "# Visualize predictions vs. actual values\n",
    "plt.scatter(data['Adj Close'],data['Predictions_Arima'])\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values\")\n",
    "plt.show()\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}\\nMean Squared Error: {mse}\\nRoot Mean Squared Error: {rmse}')\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arima Model with Time series decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sp500 = sp500_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Check for Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(row_sp500['Adj Close'].dropna())\n",
    "\n",
    "if (result[1] > 0.05):\n",
    "    print('Data is not Stationary')\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "else:\n",
    "    print('Data is Stationary')\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Remove Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sp500['Adj_Close_diff'] = row_sp500['Adj Close'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sp500= row_sp500.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = adfuller(row_sp500['Adj_Close_diff'])\n",
    "\n",
    "if (result2[1] > 0.05):\n",
    "    print('Data is not Stationary')\n",
    "    print('ADF Statistic:', result2[0])\n",
    "    print('p-value:', result2[1])\n",
    "else:\n",
    "    print('Data is Stationary')\n",
    "    print('ADF Statistic:', result2[0])\n",
    "    print('p-value:', result2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "# Create traces\n",
    "trace1 = go.Scatter(x=row_sp500.index, y=row_sp500['Adj Close'], mode='lines', name='Adjusted Close')\n",
    "trace2 = go.Scatter(x=row_sp500.index, y=row_sp500['Adj_Close_diff'], mode='lines', name='Adj_Close_diff')\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces to figure\n",
    "fig.add_trace(trace1)\n",
    "fig.add_trace(trace2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='Plot of Adjuste Close Vs. Cleaned trend Data over Time',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='USD',\n",
    "                  hovermode='x unified')\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "\n",
    "lags = 20  # Number of lags to consider\n",
    "\n",
    "# Calculate ACF\n",
    "acf_result = acf(row_sp500['Adj_Close_diff'], nlags=lags)\n",
    "\n",
    "# Print ACF results\n",
    "print(\"Autocorrelation Function (ACF):\")\n",
    "for lag, acf_value in enumerate(acf_result):\n",
    "    print(f\"Lag {lag}: {acf_value}\")\n",
    "\n",
    "# Calculate PACF\n",
    "pacf_result = pacf(row_sp500['Adj_Close_diff'], nlags=lags)\n",
    "\n",
    "# Print PACF results\n",
    "print(\"\\nPartial Autocorrelation Function (PACF):\")\n",
    "for lag, pacf_value in enumerate(pacf_result):\n",
    "    print(f\"Lag {lag}: {pacf_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_error = 1/np.sqrt(len(row_sp500))\n",
    "print(standard_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "sm.graphics.tsa.plot_acf(row_sp500['Adj_Close_diff'], lags=40, ax=ax[0])\n",
    "sm.graphics.tsa.plot_pacf(row_sp500['Adj_Close_diff'], lags=40, ax=ax[1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Auto Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto ARIMA to select optimal ARIMA parameters\n",
    "model = auto_arima(row_sp500['Adj_Close_diff'], seasonal=False, trace=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "auto_ar = pm.auto_arima(row_sp500['Adj_Close_diff'], stepwise=True, seasonal=False)\n",
    "auto_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ar.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Arima model is ARIMA(4,0,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sp500['Adj_Close_diff'] = feature_transform(row_sp500['Adj_Close_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "arima_diff = row_sp500.copy()\n",
    "adj_close_diff = truncate_before_wf(arima_diff, 200, 20)\n",
    "splits_arima_diff = walk_forward_validation(adj_close_diff, 200, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = (4, 0, 4)  # From Auto test\n",
    "adj_close_diff['Predictions_Arima'] = np.nan\n",
    "arima_results_list = []\n",
    "all_predictions = []\n",
    "\n",
    "for in_sample, out_of_sample in splits_arima_diff:\n",
    "    arima_model = ARIMA(in_sample['Adj_Close_diff'], order=order)\n",
    "    arima_results = arima_model.fit()\n",
    "    forecast = arima_results.forecast(steps=len(out_of_sample['Adj_Close_diff']))\n",
    "    arima_results_list.append((arima_results, forecast))\n",
    "    all_predictions.append(forecast)\n",
    "\n",
    "# Concatenate all predictions into a single DataFrame\n",
    "all_predictions_df = pd.DataFrame(np.concatenate(all_predictions), columns=['Predictions_Arima'])\n",
    "\n",
    "# Update the 'Predictions_Arima' column in the original DataFrame\n",
    "adj_close_diff.loc[adj_close_diff.index[200:], 'Predictions_Arima'] = all_predictions_df['Predictions_Arima'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'row_sp500' is your DataFrame with a time index\n",
    "row_sp500.index = pd.to_datetime(row_sp500.index)  # Ensure index is datetime type\n",
    "\n",
    "# Resample the data to the desired frequency\n",
    "resampled_data = row_sp500['Adj_Close_diff'].resample('D').mean().interpolate()\n",
    "\n",
    "# Perform STL decomposition\n",
    "stl_result = STL(resampled_data).fit()\n",
    "\n",
    "# Extract the components\n",
    "trend = stl_result.trend\n",
    "seasonal = stl_result.seasonal\n",
    "residual = stl_result.resid\n",
    "\n",
    "# Plot the components\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(resampled_data, label='Original')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(412)\n",
    "plt.plot(resampled_data.index, trend, label='Trend')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(413)\n",
    "plt.plot(resampled_data.index, seasonal, label='Seasonal')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(414)\n",
    "plt.plot(resampled_data.index, residual, label='Residual')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract trend, seasonal, and residual components from the original data\n",
    "detrended_data = resampled_data - trend\n",
    "deseasonalized_data = resampled_data - seasonal\n",
    "residual_data = resampled_data - trend - seasonal\n",
    "\n",
    "# Plot the detrended, deseasonalized, and residual data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.plot(detrended_data, label='Detrended Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(deseasonalized_data, label='Deseasonalized Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(residual_data, label='Residual Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Residuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot residuals and density\n",
    "residuals = arima_results.resid\n",
    "fig, ax = plt.subplots(1,2)\n",
    "residuals.plot(title='Residuals', ax=ax[0])\n",
    "residuals.plot(title='Density', kind='kde', ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame containing the data\n",
    "# Replace 'feature1' and 'feature2' with the names of the features you want to plot\n",
    "feature1 = 'Adj_Close_diff'\n",
    "feature2 = 'Predictions_Arima'\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=adj_close_diff.index,  # Assuming the index represents x-axis values\n",
    "    y=adj_close_diff[feature1],\n",
    "    mode='lines',\n",
    "    name=feature1\n",
    "))\n",
    "\n",
    "# Add another line plot for the second feature\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=adj_close_diff.index,  # Assuming the index represents x-axis values\n",
    "    y=adj_close_diff[feature2],\n",
    "    mode='lines',\n",
    "    name=feature2\n",
    "))\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Arima Model (4,0,4) - Adj Close Diff Vs. Predictions on train dataset - WF 200,20',\n",
    "    xaxis=dict(title='X Axis'),  # Customize x-axis label\n",
    "    yaxis=dict(title='Y Axis'),  # Customize y-axis label\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_close_diff = adj_close_diff.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate\n",
    "\n",
    "mae = mean_absolute_error(adj_close_diff['Adj_Close_diff'],adj_close_diff['Predictions_Arima'])\n",
    "mse = mean_squared_error(adj_close_diff['Adj_Close_diff'],adj_close_diff['Predictions_Arima'])\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(adj_close_diff['Adj_Close_diff'],adj_close_diff['Predictions_Arima'])\n",
    "mape = mean_absolute_percentage_error(adj_close_diff['Adj_Close_diff'],adj_close_diff['Predictions_Arima'])\n",
    "# Visualize predictions vs. actual values\n",
    "plt.scatter(adj_close_diff['Adj_Close_diff'],adj_close_diff['Predictions_Arima'])\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values\")\n",
    "plt.show()\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}\\nMean Squared Error: {mse}\\nRoot Mean Squared Error: {rmse}\\nR-squared: {r2}')\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data from na values & pick the columns\n",
    "knn_data = sp500.copy()\n",
    "knn_data = drop_nan(knn_data)\n",
    "knn_data = knn_data.loc[:, ['SMA','WMA', 'MACD', 'RSI', '%K_fast', '%D_fast', '%D_slow', 'Bollinger Diff',\n",
    "                    'WPR', 'OBV', 'ROC', 'ATR', 'MFI', 'Chaikin_Oscillator', 'Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "knn_data = truncate_before_wf(knn_data, 200, 20)\n",
    "splits = walk_forward_validation(knn_data, 200, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNRegressor:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "        self.oKNNreg = KNeighborsRegressor(n_neighbors=k)\n",
    "\n",
    "    def fit(self, dfX: pd.DataFrame, vY: pd.Series):\n",
    "        self.oKNNreg.fit(dfX, vY) \n",
    "\n",
    "    def predict(self, dfX: pd.DataFrame):\n",
    "        y_pred = self.oKNNreg.predict(dfX)\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, dfX: pd.DataFrame, vY: pd.Series):\n",
    "        R2 = self.oKNNreg.score(dfX, vY)\n",
    "        return R2\n",
    "    \n",
    "    def train(self, splits, col_drop):\n",
    "        predictions = []\n",
    "        for i, (in_sample, out_of_sample) in enumerate(splits):\n",
    "            X_train = in_sample.drop(columns=[col_drop])\n",
    "            y_train = in_sample[col_drop]\n",
    "            self.oKNNreg.fit(X_train, y_train)\n",
    "\n",
    "            X_test = out_of_sample.drop(columns=[col_drop])\n",
    "            predictions.extend(self.oKNNreg.predict(X_test))\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, y_true, y_pred):\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        # Visualize predictions vs. actual values\n",
    "        plt.scatter(y_true, y_pred)\n",
    "        plt.xlabel(\"Actual Values\")\n",
    "        plt.ylabel(\"Predicted Values\")\n",
    "        plt.title(\"Actual vs. Predicted Values\")\n",
    "        plt.show()\n",
    "\n",
    "        return {\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Root Mean Squared Error\": rmse,\n",
    "            \"R-squared\": r2\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Instantiate LinearSVRTrainer\n",
    "model = KNNRegressor(k=7)\n",
    "\n",
    "# Train the model\n",
    "predictions = model.train(splits,'Adj Close')\n",
    "\n",
    "# Initialize an empty list to store predictions\n",
    "index_dropped = len(knn_data)-len(predictions)\n",
    "\n",
    "knn_data_results = knn_data.copy()\n",
    "knn_data_results = knn_data_results.iloc[index_dropped:, :]\n",
    "knn_data_results['Predictions_LinearSVM'] = predictions\n",
    "\n",
    "# Instantiate LinearSVREvaluator\n",
    "evaluation_results = model.evaluate(knn_data_results['Adj Close'], knn_data_results['Predictions_LinearSVM'] )\n",
    "mape = calculate_mape(knn_data_results['Adj Close'], knn_data_results['Predictions_LinearSVM'])\n",
    "# Print evaluation results\n",
    "for metric, value in evaluation_results.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame containing the data\n",
    "# Replace 'feature1' and 'feature2' with the names of the features you want to plot\n",
    "feature1 = 'Adj Close'\n",
    "feature2 = 'Predictions_LinearSVM'\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=knn_data_results.index,  # Assuming the index represents x-axis values\n",
    "    y=knn_data_results[feature1],\n",
    "    mode='lines',\n",
    "    name=feature1\n",
    "))\n",
    "\n",
    "# Add another line plot for the second feature\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=knn_data_results.index,  # Assuming the index represents x-axis values\n",
    "    y=knn_data_results[feature2],\n",
    "    mode='lines',\n",
    "    name=feature2\n",
    "))\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='KNN K=5 - Adj Close Vs. Predictions on train dataset - WF 200,20',\n",
    "    xaxis=dict(title='X Axis'),  # Customize x-axis label\n",
    "    yaxis=dict(title='Y Axis'),  # Customize y-axis label\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class For All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesModels:\n",
    "    def __init__(self, k=5, svr_kernel='rbf', C=1.0, gamma=0.1, degree=3, rf_n_estimators=100, rf_max_features=4, \n",
    "                 rf_criterion = 'squared_error', gbm_n_estimators=100, gbm_criterion='squared_error',\n",
    "                 gbm_loss='squared_error', gbm_n_features='sqrt'):\n",
    "\n",
    "        self.k = k\n",
    "        self.svr_kernel = svr_kernel\n",
    "        self.gamma = gamma\n",
    "        self.degree = degree\n",
    "        self.C = C\n",
    "        self.rf_n_estimators = rf_n_estimators\n",
    "        self.rf_max_features = rf_max_features\n",
    "        self.rf_criterion = rf_criterion\n",
    "        self.gbm_n_estimators = gbm_n_estimators\n",
    "        self.gbm_criterion = gbm_criterion\n",
    "        self.gbm_loss = gbm_loss\n",
    "        self.gbm_n_features = gbm_n_features\n",
    "\n",
    "        # Initialize models\n",
    "        self.knn_model = KNeighborsRegressor(n_neighbors=self.k)\n",
    "        self.svr_model = SVR(kernel=self.svr_kernel, gamma=self.gamma, degree=self.degree, C=self.C)\n",
    "        self.rf_model = RandomForestRegressor(n_estimators=self.rf_n_estimators,\n",
    "                         max_features=self.rf_max_features, criterion=self.rf_criterion, bootstrap=False, warm_start=True)\n",
    "        self.gbm_model = GradientBoostingRegressor(n_estimators=self.gbm_n_estimators, \n",
    "                                                   criterion=self.gbm_criterion, loss=self.gbm_loss\n",
    "                                                   ,max_features=self.gbm_n_features)\n",
    "        self.lr_model = LinearRegression()\n",
    "\n",
    "    def knn_fit(self, dfX, vY):\n",
    "        self.knn_model.fit(dfX, vY)\n",
    "\n",
    "    def svr_fit(self, dfX, vY):\n",
    "        self.svr_model.fit(dfX, vY)\n",
    "\n",
    "    def rf_fit(self, dfX, vY):\n",
    "        self.rf_model.fit(dfX, vY)\n",
    "\n",
    "    def gbm_fit(self, dfX, vY):\n",
    "        self.gbm_model.fit(dfX, vY)\n",
    "    \n",
    "    def lr_fit(self, dfX, vY):\n",
    "        self.lr_model.fit(dfX, vY)\n",
    "\n",
    "    def knn_predict(self, dfX):\n",
    "        return self.knn_model.predict(dfX)\n",
    "\n",
    "    def svr_predict(self, dfX):\n",
    "        return self.svr_model.predict(dfX)\n",
    "\n",
    "    def rf_predict(self, dfX):\n",
    "        return self.rf_model.predict(dfX)\n",
    "\n",
    "    def gbm_predict(self, dfX):\n",
    "        return self.gbm_model.predict(dfX)\n",
    "    \n",
    "    def lr_predict(self, dfX):\n",
    "        return self.lr_model.predict(dfX)\n",
    "\n",
    "    def knn_score(self, dfX, vY):\n",
    "        return self.knn_model.score(dfX, vY)\n",
    "\n",
    "    def svr_score(self, dfX, vY):\n",
    "        return self.svr_model.score(dfX, vY)\n",
    "\n",
    "    def rf_score(self, dfX, vY):\n",
    "        return self.rf_model.score(dfX, vY)\n",
    "\n",
    "    def gbm_score(self, dfX, vY):\n",
    "        return self.gbm_model.score(dfX, vY)\n",
    "    \n",
    "    def lr_score(self, dfX, vY):\n",
    "        return self.lr_model.score(dfX, vY)\n",
    "\n",
    "    def knn_train(self, splits, col_drop):\n",
    "        predictions = []\n",
    "        \n",
    "        for i, (in_sample, out_of_sample) in enumerate(splits):\n",
    "            X_train = in_sample.drop(columns=[col_drop])\n",
    "            y_train = in_sample[col_drop]\n",
    "            self.knn_model.fit(X_train, y_train)\n",
    "\n",
    "            X_test = out_of_sample.drop(columns=[col_drop])\n",
    "            prediction = self.knn_model.predict(X_test)\n",
    "            predictions.extend(prediction)\n",
    "        return (predictions)# Return both predictions and evaluations\n",
    "    \n",
    "    def svr_train(self, splits, col_drop):\n",
    "        predictions = []\n",
    "        for i, (in_sample, out_of_sample) in enumerate(splits):\n",
    "            X_train = in_sample.drop(columns=[col_drop])\n",
    "            y_train = in_sample[col_drop]\n",
    "            self.svr_model.fit(X_train, y_train)\n",
    "\n",
    "            X_test = out_of_sample.drop(columns=[col_drop])\n",
    "            prediction = self.svr_model.predict(X_test)\n",
    "            predictions.extend(prediction)\n",
    "        return (predictions)# Return both predictions and evaluations\n",
    "    \n",
    "    def rf_train(self, splits, col_drop):\n",
    "        predictions = []\n",
    "        for i, (in_sample, out_of_sample) in enumerate(splits):\n",
    "            X_train = in_sample.drop(columns=[col_drop])\n",
    "            y_train = in_sample[col_drop]\n",
    "            self.rf_model.fit(X_train, y_train)\n",
    "\n",
    "            X_test = out_of_sample.drop(columns=[col_drop])\n",
    "            prediction = self.rf_model.predict(X_test)\n",
    "            predictions.extend(prediction)\n",
    "        return (predictions)# Return both predictions and evaluations\n",
    "\n",
    "    def gbm_train(self, splits, col_drop):\n",
    "        predictions = []\n",
    "        for i, (in_sample, out_of_sample) in enumerate(splits):\n",
    "            X_train = in_sample.drop(columns=[col_drop])\n",
    "            y_train = in_sample[col_drop]\n",
    "            self.gbm_model.fit(X_train, y_train)\n",
    "\n",
    "            X_test = out_of_sample.drop(columns=[col_drop])\n",
    "            prediction = self.gbm_model.predict(X_test)\n",
    "            predictions.extend(prediction)\n",
    "        return (predictions)# Return both predictions and evaluations\n",
    "    \n",
    "    def lr_train(self, splits, col_drop):\n",
    "        predictions = []\n",
    "        for i, (in_sample, out_of_sample) in enumerate(splits):\n",
    "            X_train = in_sample.drop(columns=[col_drop])\n",
    "            y_train = in_sample[col_drop]\n",
    "            self.lr_model.fit(X_train, y_train)\n",
    "\n",
    "            X_test = out_of_sample.drop(columns=[col_drop])\n",
    "            prediction = self.lr_model.predict(X_test)\n",
    "            predictions.extend(prediction)\n",
    "        return (predictions)# Return both predictions and evaluations\n",
    "    \n",
    "    def evaluate(self, y_true, y_pred):\n",
    "        self.mae = mean_absolute_error(y_true, y_pred)\n",
    "        self.mse = mean_squared_error(y_true, y_pred)\n",
    "        self.rmse = np.sqrt(self.mse)\n",
    "        self.r2 = r2_score(y_true, y_pred)\n",
    "        self.mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "        return (self.mae, self.mse, self.rmse, self.r2, self.mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data to perform Optemization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data from na values & pick the columns\n",
    "sp500_d = sp500.copy()\n",
    "sp500_d = drop_nan(sp500_d)\n",
    "sp500_d = sp500_d.loc[:, ['SMA','WMA', 'MACD', 'RSI', '%K_fast', '%D_fast', '%D_slow', 'Bollinger Diff',\n",
    "                    'WPR', 'OBV', 'ROC', 'ATR', 'MFI', 'Chaikin_Oscillator', 'Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "sp500_d = truncate_before_wf(sp500_d, 200, 20)\n",
    "splits = walk_forward_validation(sp500_d, 200, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search SVR Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lC      = [1, 100, 1000, 10000]\n",
    "lKernel = ['poly', 'rbf','linear']\n",
    "lgamma      = ['scale', 'auto', 0.1, 0.01, 0.001]\n",
    "ldegree = [3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the number of combinations.\n",
    "# 2. Create a nested loop to create the combinations between the parameters.\n",
    "# 3. Store the combinations as the columns of a data frame.\n",
    "\n",
    "# For Advanced Python users: Use iteration tools for create the cartesian product\n",
    "numComb = len(lKernel) * len(lC) * len(lgamma) * len(ldegree)\n",
    "dData   = {'kernel': [], 'C': [], 'gamma':[], 'degree':[]}\n",
    "\n",
    "for ii, kernel in enumerate(lKernel):\n",
    "    for jj, paramC in enumerate(lC):\n",
    "        for kk, gamma in enumerate(lgamma):\n",
    "            for cc, degree in enumerate(ldegree):\n",
    "                dData['kernel'].append(kernel)\n",
    "                dData['C'].append(paramC)\n",
    "                dData['gamma'].append(gamma)\n",
    "                dData['degree'].append(degree)\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels(svr_kernel='rbf', C=1.0, gamma=0.1, degree=3)\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 200\n",
    "vY = sp500_d['Adj Close'].values\n",
    "vY = vY[index_dropped:]\n",
    "for ii in range(numComb):\n",
    "    kernel    = dfModelScore.loc[ii, 'kernel']\n",
    "    paramC          = dfModelScore.loc[ii, 'C']\n",
    "    gamma          = dfModelScore.loc[ii, 'gamma']\n",
    "    degree          = dfModelScore.loc[ii, 'degree']\n",
    "\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb}')\n",
    "\n",
    "    Model = TimeSeriesModels(svr_kernel=kernel, C=paramC, gamma=gamma, degree=degree)\n",
    "    predictions = Model.svr_train(splits, 'Adj Close')\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mae, mse, rmse, r2, mape = Model.evaluate(vY, predictions)\n",
    "    \n",
    "    # Update the 'R2' column in dfModelScore with the calculated R2 score\n",
    "    dfModelScore.loc[ii, 'MAE'] = mae\n",
    "    dfModelScore.loc[ii, 'MSE'] = mse\n",
    "    dfModelScore.loc[ii, 'RMSE'] = rmse\n",
    "    dfModelScore.loc[ii, 'R2'] = r2\n",
    "    dfModelScore.loc[ii, 'MAPE'] = mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Best Model Score or SVR Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of MAE, MSE, RMSE, and MAPE for each model\n",
    "dfModelScore['MeanScore'] = dfModelScore[['MAE', 'MSE', 'RMSE', 'R2', 'MAPE']].mean(axis=1)\n",
    "\n",
    "# Find the index of the model with the lowest mean score\n",
    "best_model_index = dfModelScore['MeanScore'].idxmin()\n",
    "\n",
    "# Get the parameters of the best model\n",
    "best_model_params = dfModelScore.loc[best_model_index, ['kernel', 'C', 'gamma', 'degree']]\n",
    "\n",
    "# Print the parameters of the best model\n",
    "print(\"Best Model Parameters:\")\n",
    "print(best_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the best SVR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels(svr_kernel='linear', C=100, gamma='scale', degree=3)\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 200\n",
    "predictions = Model.svr_train(splits, 'Adj Close')\n",
    "\n",
    "sp500_d_includes_results = sp500_d.copy()\n",
    "sp500_d_includes_results = sp500_d_includes_results.iloc[index_dropped:, :]\n",
    "sp500_d_includes_results['Predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 200\n",
    "vY = sp500_d['Adj Close'].values\n",
    "vY = vY[index_dropped:]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae, mse, rmse, r2, mape = Model.evaluate(vY, predictions)\n",
    "\n",
    "# Update the 'R2' column in dfModelScore with the calculated R2 score\n",
    "dfModelScore.loc['MAE'] = mae\n",
    "dfModelScore.loc['MSE'] = mse\n",
    "dfModelScore.loc['RMSE'] = rmse\n",
    "dfModelScore.loc['R2'] = r2\n",
    "dfModelScore.loc['MAPE'] = mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dData   = {'K': []}\n",
    "\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels()\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 200\n",
    "vY = sp500_d['Adj Close'].values\n",
    "vY = vY[index_dropped:]\n",
    "for kk in range(10):\n",
    "    print(f'Processing model {kk + 1:03d} out of {10}')\n",
    "\n",
    "    Model = TimeSeriesModels(k=kk + 1)\n",
    "    predictions = Model.knn_train(splits, 'Adj Close')\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mae, mse, rmse, r2, mape = Model.evaluate(vY, predictions)\n",
    "    \n",
    "    # Update the 'R2' column in dfModelScore with the calculated R2 score\n",
    "    dfModelScore.loc[kk, 'k'] = kk + 1\n",
    "    dfModelScore.loc[kk, 'MAE'] = mae\n",
    "    dfModelScore.loc[kk, 'MSE'] = mse\n",
    "    dfModelScore.loc[kk, 'RMSE'] = rmse\n",
    "    dfModelScore.loc[kk, 'R2'] = r2\n",
    "    dfModelScore.loc[kk, 'MAPE'] = mape\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of MAE, MSE, RMSE, and MAPE for each model\n",
    "dfModelScore['MeanScore'] = dfModelScore[['MAE', 'MSE', 'RMSE', 'R2', 'MAPE']].mean(axis=1)\n",
    "\n",
    "# Find the index of the model with the lowest mean score\n",
    "best_model_index = dfModelScore['R2'].idxmax()\n",
    "\n",
    "# Get the parameters of the best model\n",
    "best_model_params = dfModelScore.loc[best_model_index, ['k']]\n",
    "\n",
    "# Print the parameters of the best model\n",
    "print(\"Best Model Parameters:\")\n",
    "print(best_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the best KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels(k=2)\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 200\n",
    "predictions = Model.knn_train(splits, 'Adj Close')\n",
    "\n",
    "sp500_d_includes_results = sp500_d.copy()\n",
    "sp500_d_includes_results = sp500_d_includes_results.iloc[index_dropped:, :]\n",
    "sp500_d_includes_results['Predictions'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegressor - no hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels()\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 200\n",
    "vY = sp500_d['Adj Close'].values\n",
    "vY = vY[index_dropped:]\n",
    "\n",
    "predictions = Model.lr_train(splits, 'Adj Close')\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae, mse, rmse, r2, mape = Model.evaluate(vY, predictions)\n",
    "\n",
    "# Update the 'R2' column in dfModelScore with the calculated R2 score\n",
    "dfModelScore.loc['MAE'] = mae\n",
    "dfModelScore.loc['MSE'] = mse\n",
    "dfModelScore.loc['RMSE'] = rmse\n",
    "dfModelScore.loc['R2'] = r2\n",
    "dfModelScore.loc['MAPE'] = mape\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_features = ['sqrt', 'log2', 4, 5, 6, 7]\n",
    "#criterion = ['squared_error', 'MAE']\n",
    "criterion = ['squared_error']\n",
    "n_estimators = [10, 100, 150, 200] #number of threes in the forest (100 default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the number of combinations.\n",
    "# 2. Create a nested loop to create the combinations between the parameters.\n",
    "# 3. Store the combinations as the columns of a data frame.\n",
    "\n",
    "# For Advanced Python users: Use iteration tools for create the cartesian product\n",
    "numComb = len(m_features) * len(criterion) * len(n_estimators)\n",
    "dData   = {'max_features': [], 'criterion': [], 'n_estimators':[]}\n",
    "\n",
    "for ii, feature in enumerate(m_features):\n",
    "    for jj, cri in enumerate(criterion):\n",
    "        for kk, est in enumerate(n_estimators):\n",
    "            dData['max_features'].append(feature)\n",
    "            dData['criterion'].append(cri)\n",
    "            dData['n_estimators'].append(est)\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels()\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 200\n",
    "vY = sp500_d['Adj Close'].values\n",
    "vY = vY[index_dropped:]\n",
    "for ii in range(numComb):\n",
    "    rf_n_feature = dfModelScore.loc[ii, 'max_features']\n",
    "    rf_criterion = dfModelScore.loc[ii, 'criterion']\n",
    "    rf_n_est = dfModelScore.loc[ii, 'n_estimators']\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb}')\n",
    "    Model = TimeSeriesModels(rf_n_estimators=rf_n_est, rf_max_features=rf_n_feature ,rf_criterion = rf_criterion)\n",
    "    predictions = Model.rf_train(splits, 'Adj Close')\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mae, mse, rmse, r2, mape = Model.evaluate(vY, predictions)\n",
    "    \n",
    "    # Update the 'R2' column in dfModelScore with the calculated R2 score\n",
    "    dfModelScore.loc[ii, 'MAE'] = mae\n",
    "    dfModelScore.loc[ii, 'MSE'] = mse\n",
    "    dfModelScore.loc[ii, 'RMSE'] = rmse\n",
    "    dfModelScore.loc[ii, 'R2'] = r2\n",
    "    dfModelScore.loc[ii, 'MAPE'] = mape\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of MAE, MSE, RMSE, and MAPE for each model\n",
    "dfModelScore['MeanScore'] = dfModelScore[['MAE', 'MSE', 'RMSE', 'R2', 'MAPE']].mean(axis=1)\n",
    "\n",
    "# Find the index of the model with the lowest mean score\n",
    "best_model_index = dfModelScore['RMSE'].idxmin()\n",
    "#best_model_index = dfModelScore['R2'].idxmax()\n",
    "\n",
    "# Get the parameters of the best model\n",
    "best_model_params = dfModelScore.loc[best_model_index, ['max_features', 'criterion', 'n_estimators']]\n",
    "\n",
    "# Print the parameters of the best model\n",
    "print(\"Best Model Parameters:\")\n",
    "print(best_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelScore.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the best Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels(rf_n_estimators=150, rf_max_features=7)\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 200\n",
    "predictions = Model.rf_train(splits, 'Adj Close')\n",
    "\n",
    "sp500_d_includes_results = sp500_d.copy()\n",
    "sp500_d_includes_results = sp500_d_includes_results.iloc[index_dropped:, :]\n",
    "sp500_d_includes_results['Predictions'] = predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_loss = ['squared_error', 'absolute_error', 'huber']\n",
    "L_criterion = ['friedman_mse', 'squared_error']\n",
    "L_n_estimators = [100, 150, 200] #number of threes in the forest (100 default)\n",
    "L_m_features = ['sqrt', 'log2', 5, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the number of combinations.\n",
    "# 2. Create a nested loop to create the combinations between the parameters.\n",
    "# 3. Store the combinations as the columns of a data frame.\n",
    "\n",
    "# For Advanced Python users: Use iteration tools for create the cartesian product\n",
    "numComb = len(L_loss) * len(L_criterion) * len(L_n_estimators) *len(L_m_features)\n",
    "dData   = {'loss': [], 'criterion': [], 'n_estimators':[], 'm_features':[]}\n",
    "\n",
    "for ii, lss in enumerate(L_loss):\n",
    "    for jj, cri in enumerate(L_criterion):\n",
    "        for kk, est in enumerate(L_n_estimators):\n",
    "            for kk, feature in enumerate(L_m_features):\n",
    "                dData['loss'].append(lss)\n",
    "                dData['criterion'].append(cri)\n",
    "                dData['n_estimators'].append(est)\n",
    "                dData['m_features'].append(feature)\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels()\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 200\n",
    "vY = sp500_d['Adj Close'].values\n",
    "vY = vY[index_dropped:]\n",
    "for ii in range(numComb):\n",
    "    gbm_lo = dfModelScore.loc[ii, 'loss']\n",
    "    gbm_cri = dfModelScore.loc[ii, 'criterion']\n",
    "    gbm_n_est = dfModelScore.loc[ii, 'n_estimators']\n",
    "    gbm_feat = dfModelScore.loc[ii, 'm_features']\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb}')\n",
    "    Model = TimeSeriesModels(gbm_n_estimators=gbm_n_est, gbm_criterion=gbm_cri, gbm_loss=gbm_lo\n",
    "                                                   ,gbm_n_features=gbm_feat)\n",
    "    predictions = Model.gbm_train(splits, 'Adj Close')\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mae, mse, rmse, r2, mape = Model.evaluate(vY, predictions)\n",
    "    \n",
    "    # Update the 'R2' column in dfModelScore with the calculated R2 score\n",
    "    dfModelScore.loc[ii, 'MAE'] = mae\n",
    "    dfModelScore.loc[ii, 'MSE'] = mse\n",
    "    dfModelScore.loc[ii, 'RMSE'] = rmse\n",
    "    dfModelScore.loc[ii, 'R2'] = r2\n",
    "    dfModelScore.loc[ii, 'MAPE'] = mape\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of MAE, MSE, RMSE, and MAPE for each model\n",
    "dfModelScore['MeanScore'] = dfModelScore[['MAE', 'MSE', 'RMSE', 'R2', 'MAPE']].mean(axis=1)\n",
    "\n",
    "# Find the index of the model with the lowest mean score\n",
    "#best_model_index = dfModelScore['MeanScore'].idxmin()\n",
    "best_model_index = dfModelScore['R2'].idxmax()\n",
    "\n",
    "# Get the parameters of the best model\n",
    "best_model_params = dfModelScore.loc[best_model_index, ['loss', 'criterion', 'n_estimators', 'm_features']]\n",
    "\n",
    "# Print the parameters of the best model\n",
    "print(\"Best Model Parameters:\")\n",
    "print(best_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels(gbm_n_estimators=100, gbm_criterion='squared_error', \n",
    "                         gbm_loss='squared_error', gbm_n_features=7)\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 200\n",
    "predictions = Model.gbm_train(splits, 'Adj Close')\n",
    "\n",
    "sp500_d_includes_results = sp500_d.copy()\n",
    "sp500_d_includes_results = sp500_d_includes_results.iloc[index_dropped:, :]\n",
    "sp500_d_includes_results['Predictions'] = predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs. actual values\n",
    "plt.scatter(sp500_d_includes_results['Adj Close'], sp500_d_includes_results['Predictions'] )\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values\")\n",
    "plt.show()\n",
    "\n",
    "mae = mean_absolute_error(sp500_d_includes_results['Adj Close'], sp500_d_includes_results['Predictions'] )\n",
    "mse = mean_squared_error(sp500_d_includes_results['Adj Close'], sp500_d_includes_results['Predictions'] )\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(sp500_d_includes_results['Adj Close'], sp500_d_includes_results['Predictions'] )\n",
    "mape = mean_absolute_percentage_error(sp500_d_includes_results['Adj Close'], sp500_d_includes_results['Predictions'] )\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame containing the data\n",
    "# Replace 'feature1' and 'feature2' with the names of the features you want to plot\n",
    "feature1 = 'Adj Close'\n",
    "feature2 = 'Predictions'\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=sp500_d_includes_results.index,  # Assuming the index represents x-axis values\n",
    "    y=sp500_d_includes_results[feature1],\n",
    "    mode='lines',\n",
    "    name=feature1\n",
    "))\n",
    "\n",
    "# Add another line plot for the second feature\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sp500_d_includes_results.index,  # Assuming the index represents x-axis values\n",
    "    y=sp500_d_includes_results[feature2],\n",
    "    mode='lines',\n",
    "    name=feature2\n",
    "))\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Model - log_return Vs. Predictions on train dataset - WF 200,20',\n",
    "    xaxis=dict(title='X Axis'),  # Customize x-axis label\n",
    "    yaxis=dict(title='Y Axis'),  # Customize y-axis label\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Predected values to returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = sp500_d_includes_results.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = calculate_returns(analyze, 'Adj Close', 'Returns')\n",
    "analyze = calculate_returns(analyze, 'Predictions', 'Predicted Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_distribution(analyze, 'Predicted Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1, q2, q3 = calculate_quantiles(analyze, 'Predicted Returns')\n",
    "\n",
    "print(\"Q1 (25th percentile):\\n\", q1)\n",
    "print(\"\\nQ2 (50th percentile - Median):\\n\", q2)\n",
    "print(\"\\nQ3 (75th percentile):\\n\", q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_signal(analyze, 'Returns', 'Signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_signal(analyze, 'Predicted Returns', 'Predicted Signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = 'Returns'\n",
    "feature2 = 'Predicted Returns'\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add line plot for feature1\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=analyze.index,  # Assuming the index represents x-axis values\n",
    "    y=analyze[feature1],\n",
    "    mode='lines',\n",
    "    name=feature1\n",
    "))\n",
    "\n",
    "# Add line plot for feature2\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=analyze.index,  # Assuming the index represents x-axis values\n",
    "    y=analyze[feature2],\n",
    "    mode='lines',\n",
    "    name=feature2\n",
    "))\n",
    "\n",
    "# Calculate quartiles for feature1\n",
    "q1_feature1 = analyze[feature1].quantile(0.25)\n",
    "q3_feature1 = analyze[feature1].quantile(0.75)\n",
    "\n",
    "# Calculate quartiles for feature2\n",
    "q1_feature2 = analyze[feature2].quantile(0.25)\n",
    "q3_feature2 = analyze[feature2].quantile(0.75)\n",
    "\n",
    "# Add horizontal lines for quartiles for feature1\n",
    "fig.add_hline(y=q1_feature1, line_dash=\"dash\", line_color=\"green\", annotation_text=f'{feature1} Q1: {q1_feature1}', annotation_position=\"bottom right\")\n",
    "fig.add_hline(y=q3_feature1, line_dash=\"dash\", line_color=\"orange\", annotation_text=f'{feature1} Q3: {q3_feature1}', annotation_position=\"top right\")\n",
    "\n",
    "# Add horizontal lines for quartiles for feature2\n",
    "fig.add_hline(y=q1_feature2, line_dash=\"dash\", line_color=\"black\", annotation_text=f'{feature2} Q1: {q1_feature2}', annotation_position=\"bottom left\")\n",
    "fig.add_hline(y=q3_feature2, line_dash=\"dash\", line_color=\"white\", annotation_text=f'{feature2} Q3: {q3_feature2}', annotation_position=\"top left\")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Model - Predicted Returns Vs. Returns - WF 200,20',\n",
    "    xaxis=dict(title='Time Index'),  # Customize x-axis label\n",
    "    yaxis=dict(title='Returns'),  # Customize y-axis label\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharp_true = calculate_sharpe_ratio(analyze, 'Returns')\n",
    "sharp_pred = calculate_sharpe_ratio(analyze, 'Predicted Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharp_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_data_n = yf.download('^GSPC', start='2024-01-01', end='2024-04-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try \n",
    "dfX = sp500_data_n.copy()\n",
    "dfX = technical_indicators(dfX)\n",
    "dfX.iloc[:, 6:] = dfX.iloc[:, 6:].apply(feature_transform, axis=0)\n",
    "dfX = drop_nan(dfX)\n",
    "dfX = dfX.loc[:, ['SMA','WMA', 'MACD', 'RSI', '%K_fast', '%D_fast', '%D_slow', 'Bollinger Diff',\n",
    "                    'WPR', 'OBV', 'ROC', 'ATR', 'MFI', 'Chaikin_Oscillator', 'Adj Close']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dfX = dfX.drop(columns='Adj Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfX['Predicted Adj Close'] = Model.svr_predict(feat_dfX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame containing the data\n",
    "# Replace 'feature1' and 'feature2' with the names of the features you want to plot\n",
    "feature1 = 'Adj Close'\n",
    "feature2 = 'Predicted Adj Close'\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=dfX.index,  # Assuming the index represents x-axis values\n",
    "    y=dfX[feature1],\n",
    "    mode='lines',\n",
    "    name=feature1\n",
    "))\n",
    "\n",
    "# Add another line plot for the second feature\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=dfX.index,  # Assuming the index represents x-axis values\n",
    "    y=dfX[feature2],\n",
    "    mode='lines',\n",
    "    name=feature2\n",
    "))\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Model - log_return Vs. Predictions',\n",
    "    xaxis=dict(title='X Axis'),  # Customize x-axis label\n",
    "    yaxis=dict(title='Y Axis'),  # Customize y-axis label\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = calculate_returns(dfX, 'Adj Close', 'Returns')\n",
    "dfX = calculate_returns(dfX, 'Predicted Adj Close', 'Predicted Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_signal(analyze, 'Predicted Returns', 'Predicted Signal')\n",
    "calculate_signal(analyze, 'Returns', 'Signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = 'Returns'\n",
    "feature2 = 'Predicted Returns'\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add line plot for feature1\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=dfX.index,  # Assuming the index represents x-axis values\n",
    "    y=dfX[feature1],\n",
    "    mode='lines',\n",
    "    name=feature1\n",
    "))\n",
    "\n",
    "# Add line plot for feature2\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=dfX.index,  # Assuming the index represents x-axis values\n",
    "    y=dfX[feature2],\n",
    "    mode='lines',\n",
    "    name=feature2\n",
    "))\n",
    "\n",
    "# Calculate quartiles for feature1\n",
    "q1_feature1 = dfX[feature1].quantile(0.25)\n",
    "q3_feature1 = dfX[feature1].quantile(0.75)\n",
    "\n",
    "# Calculate quartiles for feature2\n",
    "q1_feature2 = dfX[feature2].quantile(0.25)\n",
    "q3_feature2 = dfX[feature2].quantile(0.75)\n",
    "\n",
    "# Add horizontal lines for quartiles for feature1\n",
    "fig.add_hline(y=q1_feature1, line_dash=\"dash\", line_color=\"green\", annotation_text=f'{feature1} Q1: {q1_feature1}', annotation_position=\"bottom right\")\n",
    "fig.add_hline(y=q3_feature1, line_dash=\"dash\", line_color=\"orange\", annotation_text=f'{feature1} Q3: {q3_feature1}', annotation_position=\"top right\")\n",
    "\n",
    "# Add horizontal lines for quartiles for feature2\n",
    "fig.add_hline(y=q1_feature2, line_dash=\"dash\", line_color=\"black\", annotation_text=f'{feature2} Q1: {q1_feature2}', annotation_position=\"bottom left\")\n",
    "fig.add_hline(y=q3_feature2, line_dash=\"dash\", line_color=\"white\", annotation_text=f'{feature2} Q3: {q3_feature2}', annotation_position=\"top left\")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Model - Predicted Returns Vs. Returns - WF 200,20',\n",
    "    xaxis=dict(title='Time Index'),  # Customize x-axis label\n",
    "    yaxis=dict(title='Returns'),  # Customize y-axis label\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharp_true = calculate_sharpe_ratio(dfX, 'Returns')\n",
    "sharp_pred = calculate_sharpe_ratio(dfX, 'Predicted Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharp_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
